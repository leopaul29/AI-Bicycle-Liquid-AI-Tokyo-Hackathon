# ğŸ§  AI Bicycle Companion â€” FastAPI Server

This repository contains the **FastAPI backend** used in the _AI Bicycle Companion_ project â€” an interactive mobile app developed during the [Liquid AI x W&B x Lambda Hackathon (Tokyo, 2025)](https://hackathons.liquid.ai/), where it won ğŸ¥ˆ **2nd Place** for creativity and human-centered AI.

The server provides a lightweight **API layer** for model inference, connecting the mobile app (built with Expo/React Native) to a **multimodal language model** capable of understanding both **text** and **images**.

---

## âš™ï¸ Overview

This FastAPI app (`API.py`) runs locally and exposes two main endpoints:

| Route       | Method | Description                                                |
| ----------- | ------ | ---------------------------------------------------------- |
| `/generate` | `POST` | Generate text from a user prompt                           |
| `/vqa`      | `POST` | Visual Question Answering â€” takes an image + text question |

Both endpoints use the [**LFM2-VL Japanese Finetuned Model**](https://huggingface.co/HayatoHongo/lfm2-vl-ja-finetuned-enmt1ep-jamt10eponall-vqa), a multimodal large language model trained for Japanese vision-language understanding.

---

## ğŸ§© Tech Stack

| Component        | Technology                                                                    |
| ---------------- | ----------------------------------------------------------------------------- |
| Framework        | [FastAPI](https://fastapi.tiangolo.com/)                                      |
| Model            | [Transformers (Hugging Face)](https://huggingface.co/docs/transformers/index) |
| Image Processing | Pillow (`PIL`)                                                                |
| Inference        | `torch`                                                                       |
| Model            | `HayatoHongo/lfm2-vl-ja-finetuned-enmt1ep-jamt10eponall-vqa`                  |

---

## ğŸ—‚ï¸ File Structure

```
/server
 â”œâ”€â”€ API.py          # FastAPI server file
 â”œâ”€â”€ requirements.txt
 â””â”€â”€ README.md
```

---

## ğŸ“¦ Installation & Setup

```bash
# 1. Clone the repository
git clone https://github.com/<your_username>/<your_repo>.git
cd <your_repo>/server

# 2. Create a virtual environment
python -m venv venv
source venv/bin/activate     # (on Windows: venv\Scripts\activate)

# 3. Install dependencies
pip install -r requirements.txt

# 4. Run the FastAPI server
uvicorn API:app --host 0.0.0.0 --port 8000 --reload
```

The server will be available at
ğŸ‘‰ **[http://localhost:8000](http://localhost:8000)**

---

## ğŸ§ª API Endpoints

### ğŸ”¤ `/generate` â€” Text Generation

**Request (JSON):**

```json
{
	"prompt": "ä»Šæ—¥ã®å¤©æ°—ã¯ã©ã†ã§ã™ã‹ï¼Ÿ",
	"max_new_tokens": 200,
	"temperature": 0.7
}
```

**Response:**

```json
{
	"prompt": "ä»Šæ—¥ã®å¤©æ°—ã¯ã©ã†ã§ã™ã‹ï¼Ÿ",
	"generated_text": "ä»Šæ—¥ã¯æ™´ã‚Œã§ã€è‡ªè»¢è»Šã«ã¯æœ€é«˜ã®æ—¥ã§ã™ï¼"
}
```

---

### ğŸ–¼ï¸ `/vqa` â€” Visual Question Answering

**Request (FormData):**

```
file: <image file>
question: ã“ã®å†™çœŸã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„
max_new_tokens: 200
```

**Example cURL:**

```bash
curl -X POST "http://localhost:8000/vqa" \
  -F "file=@bicycle.jpg" \
  -F "question=ã“ã®å†™çœŸã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„" \
  -F "max_new_tokens=200"
```

**Response:**

```json
{
	"question": "ã“ã®å†™çœŸã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
	"answer": "é’ã„è‡ªè»¢è»ŠãŒé“è·¯ã®æ¨ªã«æ­¢ã¾ã£ã¦ã„ã¾ã™ã€‚"
}
```

---

## âš™ï¸ Model Details

The model is automatically downloaded via `transformers`:

```python
MODEL_PATH = "HayatoHongo/lfm2-vl-ja-finetuned-enmt1ep-jamt10eponall-vqa"
```

It uses:

- `AutoProcessor` for text/image preprocessing
- `AutoModelForImageTextToText` for multimodal generation
- `torch.device("auto")` for GPU/CPU handling

No environment variables or external APIs are required â€”
all inference runs **locally** once the model is cached by Hugging Face.

---

## ğŸ§  Example Workflow

```mermaid
sequenceDiagram
    participant A as ğŸ“± Mobile App (React Native)
    participant S as ğŸ FastAPI Server
    participant M as ğŸ§© Local LFM2-VL Model

    A->>S: POST /generate (text prompt)
    S->>M: Process with LFM2-VL model
    M-->>S: Generated response
    S-->>A: Return JSON with text reply

    A->>S: POST /vqa (image + question)
    S->>M: Encode image + text
    M-->>S: Generate answer
    S-->>A: Return JSON { question, answer }
```

---

## ğŸ” Example Usage in React Native

```javascript
const response = await fetch("http://<YOUR_LOCAL_IP>:8000/generate", {
	method: "POST",
	headers: { "Content-Type": "application/json" },
	body: JSON.stringify({
		prompt: transcriptionText,
		max_new_tokens: 256,
		temperature: 0.3,
	}),
});
const result = await response.json();
console.log(result.generated_text);
```

---

## ğŸ§° Troubleshooting

| Issue                        | Cause              | Solution                                                |
| ---------------------------- | ------------------ | ------------------------------------------------------- |
| `ModuleNotFoundError: 'API'` | Wrong filename     | Ensure file is named `API.py` and run `uvicorn API:app` |
| `torch not found`            | Missing dependency | Run `pip install torch`                                 |
| Model download too slow      | Large checkpoint   | Preload the model via `transformers-cli`                |
| Network error on mobile      | Wrong IP           | Use your **local IPv4** instead of `localhost`          |

---

## ğŸ§± Requirements

```
fastapi
uvicorn
torch
transformers
Pillow
```

---

## ğŸ† Credits

Developed by

- **[@Leo-Paul MARTIN](https://github.com/leopaul29)** (Mobile & Server Integration)
- **[@Hayato Hongo](https://huggingface.co/HayatoHongo)** (Model training & tuning)
- **[@Rikka Botan](https://huggingface.co/RikkaBotan)** (Model quantization and GGUF version)

For the **[Liquid AI x W&B x Lambda Hackathon (Tokyo 2025)](https://hackathons.liquid.ai/)**
â†’ _2nd Place â€“ Human-Centered AI Design_

---

## ğŸ‡¯ğŸ‡µ Future Work

- Add speech-to-text pre-processing (Whisper)
- Add streaming inference for real-time chat
- Dockerize for deployment
- Extend to multimodal conversation memory
